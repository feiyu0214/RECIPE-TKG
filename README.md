# RECIPE-TKG: Reasoning-Centric Contrastive Fine-Tuning for Temporal Knowledge Graph Completion

This repository contains code, models, and data for experiments on temporal knowledge graph (TKG) completion using large language models (LLMs). We focus on enhancing reasoning via rule-based multi-hop history sampling and contrastive fine-tuning over datasets like ICEWS14, ICEWS18, GDELT, and YAGO.

---

## ğŸ”§ Directory Structure

### ğŸ“ `output/`
Stores the inference results for each experiment. Folder naming convention:


#### Data Sampling Shorthands:
- `tlr`: Temporal Logical Rule-based Sampling  
- `isi`: ICL-style Basic Sampling (as used in the ICL paper)  
- `rbmh`: Rule-Based Multi-Hop Sampling (our proposed method)

Each output subdirectory contains:
- `final.txt`: Cleaned inference results â€” final 10 predictions per test sample
- `raw.txt`: Unfiltered/raw output of the model
- `metric_results.txt`: Evaluation metrics (Hits@1, Hits@3, Hits@10)

#### Example Directories:
- `icews14_llama2_contrastive_rbmh_filtered`
- `icews14_llama2_semantic_diff_rbmh`
- `icews14_llama2chat_icl_tlr`
- `icews14_llama3_icl_rbmh`

---

### ğŸ“ `model/`
Each subdirectory contains LoRA fine-tuned adapters. Directory naming follows the same format as `output/`.

Example:
```bash
model/
â”œâ”€â”€ icews14_llama2_contrastive_rbmh/
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””
```

---

### ğŸ“ `data/`
Organized into training and evaluation subsets:
```bash
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ icews14/
â”‚   â”œâ”€â”€ icews18/
â”‚   â”œâ”€â”€ gdelt/
â”‚   â””â”€â”€ yago/
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ icews14/
â”‚   â”œâ”€â”€ icews18/
â”‚   â”œâ”€â”€ gdelt/
â”‚   â””â”€â”€ yago/
```

Each dataset subfolder contains files sampled using `rbmh`, `tlr`, or `isi`.

---

## ğŸš€ Running the Code

### Fine-tuning
Run the following script to start fine-tuning:
```bash
bash train.sh
```

### Evaluation
Run the following script to start evaluation:
```bash
bash eval.sh
